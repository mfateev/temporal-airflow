# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

# Airflow with Temporal Orchestrator - Getting Started
#
# This docker-compose replaces Airflow's scheduler with Temporal for orchestration.
# DAG execution is handled by Temporal workflows instead of Airflow's scheduler.
#
# IMPORTANT: This requires building an Airflow 3.x image from this repository.
#
# Components:
#   - temporal: Temporal server (workflow orchestration)
#   - postgres: Database for Airflow metadata (connections, variables, UI state)
#   - airflow-webserver: Airflow UI
#   - airflow-dag-processor: Parses DAG files into serialized_dag table
#   - temporal-worker: Executes DAGs via Temporal workflows
#
# What's different from standard Airflow:
#   - No airflow-scheduler (Temporal handles orchestration)
#   - No celery workers (Temporal worker executes tasks)
#   - No redis (Temporal replaces the message broker)
#
# Usage:
#   # From the airflow repository root:
#   docker build -f docs/temporal/Dockerfile.temporal -t airflow-temporal:latest .
#
#   # Create project directory and copy files:
#   mkdir -p airflow-temporal/dags airflow-temporal/logs airflow-temporal/scripts
#   cp -r scripts/temporal_airflow airflow-temporal/scripts/
#   cp docs/temporal/docker-compose-temporal.yaml airflow-temporal/
#
#   # Start services:
#   cd airflow-temporal
#   docker-compose -f docker-compose-temporal.yaml up -d
#
# Access:
#   - Airflow UI: http://localhost:8080 (admin/admin)
#   - Temporal UI: http://localhost:8233
#
---
x-airflow-common:
  &airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-airflow-temporal:latest}
  environment:
    &airflow-common-env
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY:-}
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR: 'true'
    AIRFLOW__CORE__EXECUTOR: 'airflow.executors.local_executor.LocalExecutor'
    # Auth configuration (bypass login for dev)
    AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_ALL_ADMINS: 'true'
    # Temporal configuration
    TEMPORAL_ADDRESS: temporal:7233
    TEMPORAL_NAMESPACE: default
    TEMPORAL_TASK_QUEUE: airflow-tasks
  volumes:
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-.}/scripts:/opt/airflow/scripts
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    postgres:
      condition: service_healthy
    temporal:
      condition: service_healthy

services:
  # =============================================================================
  # Temporal Server (Development Mode)
  # Uses SQLite storage - data persists across restarts but not for production
  # For production, use temporalio/auto-setup with PostgreSQL
  # =============================================================================
  temporal:
    image: temporalio/admin-tools:latest
    container_name: temporal
    ports:
      - "7233:7233"   # gRPC API
      - "8233:8233"   # Web UI
    entrypoint: []
    command: ["temporal", "server", "start-dev", "--ip", "0.0.0.0", "--db-filename", "/tmp/temporal.db"]
    healthcheck:
      test: ["CMD", "temporal", "operator", "cluster", "health", "--address", "localhost:7233"]
      interval: 10s
      timeout: 10s
      start_period: 30s
      retries: 30
    restart: always

  # =============================================================================
  # PostgreSQL for Airflow Metadata
  # =============================================================================
  postgres:
    image: postgres:16
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always

  # =============================================================================
  # Airflow Initialization
  # =============================================================================
  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        echo "Creating directories..."
        mkdir -p /opt/airflow/{logs,dags,scripts}

        echo "Initializing Airflow database..."
        airflow db migrate

        echo "Airflow initialization complete!"
    environment:
      <<: *airflow-common-env
    depends_on:
      postgres:
        condition: service_healthy

  # =============================================================================
  # Airflow API Server (UI)
  # Handles UI, REST API, and routes DagRuns to Temporal via orchestrator
  # =============================================================================
  airflow-apiserver:
    <<: *airflow-common
    container_name: airflow-apiserver
    command: ["airflow", "api-server"]
    ports:
      - "8080:8080"
    environment:
      <<: *airflow-common-env
      # Route all DagRuns to Temporal instead of default executor
      AIRFLOW__CORE__ORCHESTRATOR: temporal_airflow.orchestrator.TemporalOrchestrator
      PYTHONPATH: /opt/airflow/scripts
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/api/v2/monitor/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      temporal:
        condition: service_healthy

  # =============================================================================
  # Airflow DAG Processor
  # Parses DAG files and stores serialized DAGs in database
  # =============================================================================
  airflow-dag-processor:
    <<: *airflow-common
    container_name: airflow-dag-processor
    command: ["airflow", "dag-processor"]
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  # =============================================================================
  # Airflow Scheduler (OPTIONAL - only for timetable-based scheduling)
  # Evaluates DAG timetables and creates scheduled DagRuns.
  # NOT needed for manual triggers (UI/CLI/API) - those go through api-server.
  # =============================================================================
  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: ["airflow", "scheduler"]
    environment:
      <<: *airflow-common-env
      # Route all DagRuns to Temporal instead of default executor
      AIRFLOW__CORE__ORCHESTRATOR: temporal_airflow.orchestrator.TemporalOrchestrator
      PYTHONPATH: /opt/airflow/scripts
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      temporal:
        condition: service_healthy

  # =============================================================================
  # Temporal Worker for Airflow DAG Execution
  # Runs deep integration workflow + activities
  # =============================================================================
  temporal-worker:
    <<: *airflow-common
    container_name: temporal-worker
    command:
      - bash
      - -c
      - |
        pip install --quiet temporalio && python /opt/airflow/scripts/temporal_airflow/deep_worker.py
    environment:
      <<: *airflow-common-env
      PYTHONPATH: /opt/airflow/scripts
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f deep_worker.py || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      temporal:
        condition: service_healthy

  # =============================================================================
  # Airflow CLI (for debugging)
  # =============================================================================
  airflow-cli:
    <<: *airflow-common
    container_name: airflow-cli
    profiles:
      - debug
    command:
      - bash
      - -c
      - airflow
    depends_on:
      airflow-init:
        condition: service_completed_successfully

volumes:
  postgres-db-volume:
